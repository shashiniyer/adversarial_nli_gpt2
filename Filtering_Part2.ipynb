{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edbca70",
   "metadata": {},
   "source": [
    "# Dual Objective:\n",
    "- A) Use AFLite to greedily solve for $\\text{arg min}_{S \\subset \\mathcal{D}, ~|S| \\geq n}\\mathcal{R}(\\Phi, ~S, ~\\mathcal{M})$\n",
    "- B) Fine-tune GPT-2 with the resulting filtered dataset\n",
    "\n",
    "### 1. Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2TokenizerFast, DataCollatorWithPadding, set_seed\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import copy\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "set_seed(42)\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78053619",
   "metadata": {},
   "source": [
    "### 2. Pre-Processing\n",
    "- Get SNLI Dataset (Train fold) and shuffle it using the same seed as used for obtaining GPT-2 based Feature Representation (see notebook [Filtering_Part1.ipynb](https://github.com/shashiniyer/adversarial_nli_gpt2/blob/main/Filtering_Part1.ipynb))\n",
    "- Remove instances without gold standard labels, i.e., label = -1\n",
    "- One-hot encoding for labels\n",
    "- Partition data 10%/90%; use the 90% as `train`\n",
    "- Tokenise train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ad078",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = load_dataset('snli', split = 'train').shuffle(seed = 42)\n",
    "snli_train = snli_train.filter(lambda x: x['label'] != -1).map( \\\n",
    "    lambda x: {'label': one_hot(torch.tensor(x['label']), 3).type(torch.float32).numpy()}, \\\n",
    "    batched = True)\n",
    "train = snli_train.select(range(int(len(snli_train)/10), len(snli_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokeniser\n",
    "# padding to left because GPT2 uses last token for prediction\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side = 'left', \\\n",
    "                                              padding = True, truncation = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # pad with 'eos' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "train = train.map(lambda x: tokenizer(x['premise'] + '|' + x['hypothesis']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12276e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only needed columns, set data format to PyTorch\n",
    "train.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6458a",
   "metadata": {},
   "source": [
    "### 3. Set up inputs for AFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the feature representation, Phi, with linear layer attached\n",
    "model = torch.load('feature_rep.pth')\n",
    "\n",
    "# move model to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# freeze all layers except the last\n",
    "num_layers = sum(1 for _ in model.parameters())\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    \n",
    "    if idx != num_layers - 1:\n",
    "        \n",
    "        # freeze\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data collator - https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "# this is a (callable) helper object that sends batches of data to the model\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding = 'max_length', \\\n",
    "                                         return_tensors = 'pt', max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c380ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters - set to match Le et al. (2020) - https://arxiv.org/abs/2002.04108\n",
    "m = 64\n",
    "n = 182000\n",
    "t = 50000\n",
    "k = 10000\n",
    "tau = 0.75\n",
    "AFLite_seeds = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for model training within AFLite implementation\n",
    "batch_size = 16 # constrained by GPU memory\n",
    "lr = 1e-5 # also set to match Le et al. (2020) - https://arxiv.org/abs/2002.04108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fc727",
   "metadata": {},
   "source": [
    "### 4. Utility Functions for AFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, dataloader, optimizer, device, npochs = 3):\n",
    "    \n",
    "    # initialise\n",
    "    curr_loss = 0\n",
    "    #prev_loss = float(\"inf\")\n",
    "    \n",
    "    # move classifier to device and set it in train mode\n",
    "    classifier.to(device)\n",
    "    classifier.train()\n",
    "    \n",
    "    # cache training data size\n",
    "    train_data_size = len(dataloader.dataset)\n",
    "    \n",
    "    # train until convergence\n",
    "    # while abs(prev_loss - curr_loss) > 1e-5:\n",
    "    \n",
    "    # train for nepochs; nepochs = 3 in Le et al. (2020) - https://arxiv.org/abs/2002.04108\n",
    "    for _ in range(npochs):\n",
    "        \n",
    "        # reset losses\n",
    "        #prev_loss = curr_loss\n",
    "        curr_loss = 0\n",
    "        \n",
    "        for batch, data in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "\n",
    "            # Torch requirement\n",
    "            classifier.zero_grad()\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            outputs = classifier(**data.to(device))\n",
    "            batch_loss = outputs[0]\n",
    "\n",
    "            # Backpropagation\n",
    "            batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log\n",
    "            if batch % int(len(dataloader)/10) == 0:\n",
    "                batch_loss, current = batch_loss.item(), batch * len(data['labels'])\n",
    "                print(f\"loss: {batch_loss:>7f}  [{current:>5d}/{train_data_size:>5d}]\")\n",
    "            \n",
    "            # Add up batch-loss\n",
    "            curr_loss += batch_loss * len(data['labels'])\n",
    "        \n",
    "        # Average out curr_loss\n",
    "        curr_loss /= train_data_size\n",
    "        print(f'Epoch average loss: {curr_loss}')\n",
    "        \n",
    "    # set classifier to eval model and move it to CPU\n",
    "    classifier.eval()\n",
    "    classifier.to('cpu')\n",
    "\n",
    "    # print done\n",
    "    print('Done!')\n",
    "    \n",
    "    return(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier, dataloader, device):\n",
    "    \n",
    "    classifier.to(device) # move classifier to device\n",
    "    \n",
    "    for batch, data in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_preds = classifier(**data.to(device)).logits.argmax(1)\n",
    "        \n",
    "        #print(batch_preds)\n",
    "        \n",
    "        if batch == 0:\n",
    "            \n",
    "            preds = batch_preds\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            preds = torch.cat((preds, batch_preds))\n",
    "    \n",
    "    classifier.to('cpu') # move classifier to cpu\n",
    "    \n",
    "    return(preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k(pred_scores, tau, k):\n",
    "    \n",
    "    \"\"\"\n",
    "        Select up to k instances with the highest predictability scores subject to score >= tau\n",
    "    \"\"\"\n",
    "    \n",
    "    k_idx = [] # initialise\n",
    "    pred_scores = np.array(pred_scores) # format as numpy array for subsequent steps\n",
    "    sorted_descended_idx = np.argsort(-pred_scores) # sorting because we want to select instances with high pred_score\n",
    "    sorted_pred_scores = pred_scores[sorted_descended_idx] # cache\n",
    "    \n",
    "    for idx in sorted_descended_idx:\n",
    "        \n",
    "        if pred_scores[idx] < tau:\n",
    "            \n",
    "            return(k_idx)\n",
    "        \n",
    "        elif idx == sorted_descended_idx.shape[0]:\n",
    "            \n",
    "            k_idx += idx\n",
    "        \n",
    "        elif pred_scores[idx] == pred_scores[idx + 1]:\n",
    "            \n",
    "            candidates = sorted_descended_idx[sorted_pred_scores == pred_scores[idx]]\n",
    "            max_candidates_to_select = k - len(k_idx)\n",
    "            \n",
    "            if max_candidates_to_select >= candidates.shape[0]:\n",
    "                \n",
    "                k_idx += candidates.tolist()\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # randomly select a subset of `max_candidates_to_select` candidates\n",
    "                k_idx += np.random.default_rng(42).choice(candidates, max_candidates_to_select, replace = False)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            k_idx += idx\n",
    "        \n",
    "        if len(k_idx) == k:\n",
    "            \n",
    "            return(k_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eef5f",
   "metadata": {},
   "source": [
    "### 5.  AFLite Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25189fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_datasets = {}\n",
    "\n",
    "for seed in AFLite_seeds:\n",
    "    \n",
    "    # first step of AFLite; initialise S\n",
    "    S = copy.deepcopy(train)\n",
    "\n",
    "    while len(S) > n:\n",
    "        \n",
    "        # initialise multiset for Out-Of-Sample predictions\n",
    "        E = {x: [] for x in range(len(S))}\n",
    "\n",
    "        for j in range(m):\n",
    "            \n",
    "            # randomly partition S into (S\\T_j, T_j) s.t. |S\\T_j| = t\n",
    "            tr_idx = set(np.random.default_rng(j).choice(np.arange(len(S)), t, replace = False))\n",
    "            te_idx = set(range(len(S))) - tr_idx\n",
    "            tr, te = S.select(tr_idx), S.select(te_idx)\n",
    "            \n",
    "            print(str(seed) + '_' + str(j) + '_' + str(len(tr_idx)))\n",
    "            #break\n",
    "                        \n",
    "            # train classifier on S\\T_j, i.e. tr\n",
    "            classifier = copy.deepcopy(model)\n",
    "            dataloader = torch.utils.data.DataLoader(tr, batch_size=batch_size, \\\n",
    "                                 shuffle=True, collate_fn=data_collator)\n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr = lr)\n",
    "            trained_classifier = train_classifier(classifier, dataloader, optimizer, device)\n",
    "            print('Done!')\n",
    "            \n",
    "            # for all instances i in T_j, add predictions to E(i)\n",
    "            te_dataloader = torch.utils.data.DataLoader(te, batch_size=batch_size, collate_fn=data_collator)\n",
    "            preds = predict(trained_classifier, te_dataloader, device)\n",
    "            \n",
    "            for pred_idx, data_idx in enumerate(te_idx): # there are as many predictions as test instances\n",
    "                \n",
    "                E[data_idx] += [preds[pred_idx]]\n",
    "        \n",
    "        # for all instances in S, compute predictability score\n",
    "        pred_scores = [sum([1 for y_hat in x if y_hat == s[idx]['label']])/len(x) \\\n",
    "                       for idx, x in enumerate(E.values())]\n",
    "        \n",
    "        \n",
    "        # Select up to k instances with the highest predictability scores subject to score >= tau\n",
    "        selected_idx = select_k(pred_scores, tau, k)\n",
    "        \n",
    "        # filter\n",
    "        S = S.select(set(range(len(S))) - selected_idx)\n",
    "        \n",
    "        # early stopping\n",
    "        if len(selected_idx) < k:\n",
    "            \n",
    "            break\n",
    "    \n",
    "    # cache file\n",
    "    filtered_datasets[seed] = S\n",
    "    \n",
    "    # print number of instances in S, for creating random baseline\n",
    "    print(f'Number of instances in S (seed {seed}): {len(S)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8d011",
   "metadata": {},
   "source": [
    "### 6. Fine-tuning GPT-2 with AFLite filtered datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some RAM\n",
    "del train \n",
    "del model\n",
    "del classifier\n",
    "\n",
    "# Begin fine-tuning\n",
    "for seed in AFLite_seeds:\n",
    "    \n",
    "    # instantiate GPT-2 based model\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", \n",
    "                                      num_labels=3,\n",
    "                                      problem_type=\"multi_label_classification\")\n",
    "    model.config.pad_token_id = model.config.eos_token_id # specify pad_token used by tokenizer\n",
    "    \n",
    "    # set up data loader\n",
    "    dataloader = torch.utils.data.DataLoader(filtered_datasets[seed], batch_size=batch_size, \\\n",
    "                                             shuffle=True, collate_fn=data_collator)\n",
    "    \n",
    "    # set up optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)    \n",
    "    \n",
    "    # fine-tune model\n",
    "    torch.save(train_classifier(model, dataloader, optimizer, device, npochs = 3), \\\n",
    "              'AFLite_fine_tuned_model_seed' + str(seed) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ee38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
