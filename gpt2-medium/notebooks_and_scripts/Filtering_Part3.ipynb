{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aca2c3f",
   "metadata": {},
   "source": [
    "# Objective: Fine-tune GPT-2 with the resulting filtered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b36a91",
   "metadata": {},
   "source": [
    "### 1. Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941af00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import GPT2TokenizerFast, DataCollatorWithPadding, GPT2ForSequenceClassification, set_seed\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils_ import tokenize, train_classifier\n",
    "import pickle\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "set_seed(42)\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15b80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokeniser\n",
    "# padding to left because GPT2 uses last token for prediction\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2-medium\", padding_side = 'left', \\\n",
    "                                              padding = True, truncation = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # pad with 'eos' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393cf446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data collator - https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "# this is a (callable) helper object that sends batches of data to the model\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding = 'max_length', \\\n",
    "                                         return_tensors = 'pt', max_length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d163a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in indices removed by AFLite for each seed\n",
    "with open('removed_idx.pkl', 'rb') as f:\n",
    "    removed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01aee8",
   "metadata": {},
   "source": [
    "## 2. Pre-processing Routine for SNLI\n",
    "- Get SNLI Dataset (Train fold) and shuffle it using the same seed as used for obtaining GPT-2 based Feature Representation (see notebook [Filtering_Part1.ipynb](https://github.com/shashiniyer/adversarial_nli_gpt2/blob/main/gpt2-medium/notebooks_and_scripts/Filtering_Part1.ipynb))\n",
    "- Remove instances without gold standard labels, i.e., label = -1\n",
    "- One-hot encoding for labels\n",
    "- Partition data 10%/90%; use the 90% as unfiltered `train`\n",
    "- Filter out what AFLite run removed (see notebook [Filtering_Part2.ipynb](https://github.com/shashiniyer/adversarial_nli_gpt2/blob/main/gpt2-medium/notebooks_and_scripts/Filtering_Part2.ipynb))\n",
    "- Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1f79fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_snli(seed):\n",
    "    \n",
    "    # set up `train` dataset\n",
    "    snli_train = load_dataset('snli', split = 'train').shuffle(seed = 42)\n",
    "    snli_train = snli_train.filter(lambda x: x['label'] != -1).map( \\\n",
    "        lambda x: {'label': one_hot(torch.tensor(x['label']), 3).type(torch.float32).numpy()}, \\\n",
    "        batched = True)\n",
    "    train = snli_train.select(range(int(len(snli_train)/10), len(snli_train)))\n",
    "    \n",
    "    # filter out what AFLite run removed\n",
    "    removed_idx = [int(x) for x in removed[seed].split(',')[1:]] \n",
    "    train = train.select(set(range(len(train))) - set(removed_idx))\n",
    "    \n",
    "    # tokenize data\n",
    "    train = train.map(lambda x: tokenize(tokenizer, x['premise'] + '|' + x['hypothesis']))\n",
    "    len_bef_exclusion = len(train)\n",
    "\n",
    "    # exclude instances with > 128 tokens\n",
    "    train = train.filter(lambda x: x['exclude'] == False)\n",
    "    len_aft_exclusion = len(train)\n",
    "\n",
    "    # print message if instances were in fact excluded\n",
    "    if len_bef_exclusion - len_aft_exclusion > 0:\n",
    "\n",
    "        print(f'{len_bef_exclusion - len_aft_exclusion} ' + \\\n",
    "              f'({(len_bef_exclusion/len_aft_exclusion - 1)*100:>2f}%) sequences excluded')\n",
    "\n",
    "    # keep only needed columns, set data format to PyTorch\n",
    "    train.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "    return(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ac3bd",
   "metadata": {},
   "source": [
    "### 3. Run fine-tuning step and save resulting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc649eb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/shana92/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe2257057d545ec88408ee4364f078c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d15a389d4147ea8a9ed684b8b1399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e952f8dc5f4c36b66a53e1e619ab4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266415 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2269fabb08ef4daf8daf17d8f9e616b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Read In\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2fdeb3630f4000bf2cf0ea9e81e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.129973  [    0/266415]\n",
      "loss: 0.599545  [26624/266415]\n",
      "loss: 0.614978  [53248/266415]\n",
      "loss: 0.582858  [79872/266415]\n",
      "loss: 0.525758  [106496/266415]\n",
      "loss: 0.486906  [133120/266415]\n",
      "loss: 0.357164  [159744/266415]\n",
      "loss: 0.394921  [186368/266415]\n",
      "loss: 0.448161  [212992/266415]\n",
      "loss: 0.279881  [239616/266415]\n",
      "loss: 0.246570  [266240/266415]\n",
      "Epoch average loss: 0.48667386174201965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc4ff33b64430aaec3edaaaeca6beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.428860  [    0/266415]\n",
      "loss: 0.449402  [26624/266415]\n",
      "loss: 0.171800  [53248/266415]\n",
      "loss: 0.322920  [79872/266415]\n",
      "loss: 0.339696  [106496/266415]\n",
      "loss: 0.270904  [133120/266415]\n",
      "loss: 0.248552  [159744/266415]\n",
      "loss: 0.290512  [186368/266415]\n",
      "loss: 0.236461  [212992/266415]\n",
      "loss: 0.232649  [239616/266415]\n",
      "loss: 0.347984  [266240/266415]\n",
      "Epoch average loss: 0.31907910108566284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc34517ce54443d0b83d551ab8fbf3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.184027  [    0/266415]\n",
      "loss: 0.176777  [26624/266415]\n",
      "loss: 0.263505  [53248/266415]\n",
      "loss: 0.275502  [79872/266415]\n",
      "loss: 0.240916  [106496/266415]\n",
      "loss: 0.298550  [133120/266415]\n",
      "loss: 0.280404  [159744/266415]\n",
      "loss: 0.195512  [186368/266415]\n",
      "loss: 0.262465  [212992/266415]\n",
      "loss: 0.175238  [239616/266415]\n",
      "loss: 0.320463  [266240/266415]\n",
      "Epoch average loss: 0.27321189641952515\n",
      "Done!\n",
      "Seed 4 complete\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters for model training\n",
    "batch_size = 32 # constrained by GPU memory\n",
    "lr = 1e-5 # set to match Le et al. (2020) - https://arxiv.org/abs/2002.04108\n",
    "\n",
    "# from Filtering_Part2.ipynb\n",
    "AFLite_seeds = [0, 1, 2, 3, 4]\n",
    "    \n",
    "# Begin fine-tuning\n",
    "for seed in AFLite_seeds:\n",
    "    \n",
    "    # print log message\n",
    "    print(f'Seed {seed} begin')\n",
    "    \n",
    "    # read in filtered data\n",
    "    data = preprocess_snli(seed)\n",
    "    print('Filtered Data Read In')\n",
    "    \n",
    "    # instantiate new GPT-2 based model\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(\"gpt2-medium\", \n",
    "                                      num_labels=3,\n",
    "                                      problem_type=\"multi_label_classification\")\n",
    "    model.config.pad_token_id = model.config.eos_token_id # specify pad_token used by tokenizer\n",
    "    \n",
    "    # set up data loader\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, \\\n",
    "                                             shuffle=True, collate_fn=data_collator)\n",
    "    \n",
    "    # set up optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)    \n",
    "    \n",
    "    # fine-tune model\n",
    "    torch.save(train_classifier(model, dataloader, optimizer, device), \\\n",
    "              'AFLite_fine_tuned_model_seed' + str(seed) + '.pth')\n",
    "    \n",
    "    # print log message\n",
    "    print(f'Seed {seed} complete')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df423765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
