{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8edbca70",
   "metadata": {},
   "source": [
    "# Objective: Use AFLite to greedily solve for $\\text{arg min}_{S \\subset \\mathcal{D}, ~|S| \\geq n}\\mathcal{R}(\\Phi, ~S, ~\\mathcal{M})$\n",
    "\n",
    "### 1. Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfebceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import GPT2TokenizerFast, DataCollatorWithPadding, set_seed\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils_ import tokenize, train_classifier, predict, select_k\n",
    "import pickle\n",
    "import itertools\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "set_seed(42)\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78053619",
   "metadata": {},
   "source": [
    "### 2. Pre-Processing\n",
    "- Get SNLI Dataset (Train fold) and shuffle it using the same seed as used for obtaining GPT-2 based Feature Representation (see notebook [Filtering_Part1.ipynb](https://github.com/shashiniyer/adversarial_nli_gpt2/blob/main/Filtering_Part1.ipynb))\n",
    "- Remove instances without gold standard labels, i.e., label = -1\n",
    "- One-hot encoding for labels\n",
    "- Partition data 10%/90%; use the 90% as `train`\n",
    "- Tokenise train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62ad078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/shana92/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c831388287e64dc9994a8ce04871ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1f0a15bb1945fe8d6c977c62a06f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snli_train = load_dataset('snli', split = 'train').shuffle(seed = 42)\n",
    "snli_train = snli_train.filter(lambda x: x['label'] != -1).map( \\\n",
    "    lambda x: {'label': one_hot(torch.tensor(x['label']), 3).type(torch.float32).numpy()}, \\\n",
    "    batched = True)\n",
    "train = snli_train.select(range(int(len(snli_train)/10), len(snli_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9d6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokeniser\n",
    "# padding to left because GPT2 uses last token for prediction\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2-medium\", padding_side = 'left', \\\n",
    "                                              padding = True, truncation = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # pad with 'eos' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2820639c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4499594a6e449daae2737d1314358d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/494431 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bf01b5d9f44dbba26493d316e78e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize data\n",
    "train = train.map(lambda x: tokenize(tokenizer, x['premise'] + '|' + x['hypothesis']))\n",
    "len_bef_exclusion = len(train)\n",
    "\n",
    "# exclude instances with > 128 tokens\n",
    "train = train.filter(lambda x: x['exclude'] == False)\n",
    "len_aft_exclusion = len(train)\n",
    "\n",
    "# print message if instances were in fact excluded\n",
    "if len_bef_exclusion - len_aft_exclusion > 0:\n",
    "    \n",
    "    print(f'{len_bef_exclusion - len_aft_exclusion} ' + \\\n",
    "          f'({(len_bef_exclusion/len_aft_exclusion - 1)*100:>2f}%) sequences excluded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12276e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only needed columns, set data format to PyTorch\n",
    "train.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6458a",
   "metadata": {},
   "source": [
    "### 3. Set up inputs for AFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7048b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the feature representation, Phi, with linear layer attached\n",
    "model = torch.load('feature_rep.pth')\n",
    "\n",
    "# move model to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# freeze all layers except the last\n",
    "num_layers = sum(1 for _ in model.parameters())\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    \n",
    "    if idx != num_layers - 1:\n",
    "        \n",
    "        # freeze\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e87074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data collator - https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "# this is a (callable) helper object that sends batches of data to the model\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding = 'max_length', \\\n",
    "                                         return_tensors = 'pt', max_length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c380ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters - constrained by training time available\n",
    "m = 30\n",
    "n = 195000\n",
    "t = 50000\n",
    "k = 100000\n",
    "tau = 0.75\n",
    "AFLite_seeds = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ff63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for model training within AFLite implementation\n",
    "batch_size = 128 # constrained by GPU memory\n",
    "lr = 1e-5 # set to match Le et al. (2020) - https://arxiv.org/abs/2002.04108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eef5f",
   "metadata": {},
   "source": [
    "### 4.  AFLite Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25189fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 - Iteration 1 - Model 1 - Begin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a794bd01746244f791835bbe5e7a2e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.351462  [    0/50000]\n",
      "loss: 0.339478  [ 4992/50000]\n",
      "loss: 0.323733  [ 9984/50000]\n",
      "loss: 0.371513  [14976/50000]\n",
      "loss: 0.327074  [19968/50000]\n",
      "loss: 0.414310  [24960/50000]\n",
      "loss: 0.360435  [29952/50000]\n",
      "loss: 0.316903  [34944/50000]\n",
      "loss: 0.329192  [39936/50000]\n",
      "loss: 0.394657  [44928/50000]\n",
      "loss: 0.372505  [31200/50000]\n",
      "Epoch average loss: 0.36408787965774536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b578ece3802a440ba88b5a245027f38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.384505  [    0/50000]\n",
      "loss: 0.419292  [ 4992/50000]\n",
      "loss: 0.439487  [ 9984/50000]\n",
      "loss: 0.422065  [14976/50000]\n",
      "loss: 0.387640  [19968/50000]\n"
     ]
    }
   ],
   "source": [
    "# set up containers to collect outputs\n",
    "filtered_datasets = {}\n",
    "removed_idx = {x: '' for x in AFLite_seeds}\n",
    "\n",
    "# begin procedure\n",
    "for seed in AFLite_seeds:\n",
    "    \n",
    "    # first step of AFLite; initialise S\n",
    "    S = copy.deepcopy(train)\n",
    "    \n",
    "    # initialise iteration index\n",
    "    it_idx = 0\n",
    "    \n",
    "    while len(S) > n:\n",
    "        \n",
    "        # update iteration index\n",
    "        it_idx += 1\n",
    "        \n",
    "        # initialise multiset for Out-Of-Sample predictions\n",
    "        E = {x: [] for x in range(len(S))}\n",
    "\n",
    "        for j in range(m):\n",
    "            \n",
    "            # randomly partition S into (S\\T_j, T_j) s.t. |S\\T_j| = t\n",
    "            tr_idx = set(np.random.default_rng(j).choice(np.arange(len(S)), t, replace = False))\n",
    "            te_idx = set(range(len(S))) - tr_idx\n",
    "            tr, te = S.select(tr_idx), S.select(te_idx)\n",
    "            print(f'Seed {seed} - Iteration {it_idx} - Model {j + 1} - Begin')\n",
    "                        \n",
    "            # train classifier on S\\T_j, i.e. tr\n",
    "            classifier = copy.deepcopy(model)\n",
    "            dataloader = torch.utils.data.DataLoader(tr, batch_size=batch_size, \\\n",
    "                                 shuffle=True, collate_fn=data_collator)\n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr = lr)\n",
    "            trained_classifier = train_classifier(classifier, dataloader, optimizer, device)\n",
    "            \n",
    "            # for all instances i in T_j, add predictions to E(i)\n",
    "            te_dataloader = torch.utils.data.DataLoader(te, batch_size=batch_size, collate_fn=data_collator)\n",
    "            preds = predict(trained_classifier, te_dataloader, device)\n",
    "            print(f'Seed {seed} - Iteration {it_idx} - Model {j + 1} - Done')\n",
    "            \n",
    "            for pred_idx, data_idx in enumerate(te_idx): # there are as many predictions as test instances\n",
    "                \n",
    "                E[data_idx] += [preds[pred_idx]]\n",
    "        \n",
    "        # for all instances in S, compute predictability score\n",
    "        # in the corner case that there are no predictions for an instance, we do not filter it out\n",
    "        lengths = torch.tensor([len(x) if len(x) > 0 else 1 for x in E.values()])\n",
    "        preds_padded = torch.tensor(list(itertools.zip_longest(*E.values(), fillvalue=-1))).transpose(0, 1)\n",
    "        labels = torch.repeat_interleave(S['label'].argmax(1), max(lengths)).reshape(preds_padded.size())\n",
    "\n",
    "        pred_matches = torch.eq(preds_padded, labels)\n",
    "        pred_match_totals = torch.sum(pred_matches, axis = 1)\n",
    "        pred_scores = pred_match_totals / lengths\n",
    "        \n",
    "        # select up to k instances with the highest predictability scores subject to score >= tau\n",
    "        selected_idx = select_k(pred_scores, tau, k, seed)\n",
    "        \n",
    "        if selected_idx.shape[0] > 0:\n",
    "        \n",
    "            # cache instances selected for removal\n",
    "            removed_idx[seed] += ',' + ','.join([str(idx) for idx in selected_idx])\n",
    "\n",
    "            # filter out selected instances\n",
    "            S = S.select(set(range(len(S))) - set(selected_idx))\n",
    "        \n",
    "        # early stopping\n",
    "        elif selected_idx.shape[0] < k:\n",
    "            \n",
    "            break\n",
    "    \n",
    "    # cache file\n",
    "    filtered_datasets[seed] = S\n",
    "    \n",
    "    # print number of instances in S, for creating random baseline\n",
    "    print(f'Number of instances in S (seed {seed}): {len(S)}')\n",
    "    \n",
    "# write out list of removed indices for further analysis\n",
    "with open('removed_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(removed_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out list of removed indices for further analysis\n",
    "with open('removed_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(removed_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c289bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
