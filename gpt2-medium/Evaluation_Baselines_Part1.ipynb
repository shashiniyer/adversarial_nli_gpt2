{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6016ff83",
   "metadata": {},
   "source": [
    "# Evaluate Performance of Baseline Models\n",
    "__Model performance will be evaluated on:__\n",
    "1. In-Distribution sample (SNLI test split), in zero-shot settings (_this notebook_)\n",
    "2. The following Out-of-Distribution samples:\n",
    "    - HANS dataset (validation split), in zero-shot settings (_this notebook_)\n",
    "    - NLI Diagnostics dataset, in zero-shot settings (_this notebook_)\n",
    "    - Stress Test datasets, in zero-shot settings (_this notebook_)\n",
    "    - ANLI datasets (test splits), after fine-tuning the model for each round (see [Part 2](https://github.com/shashiniyer/adversarial_nli_gpt2/blob/main/gpt2-medium/Evaluation_Baselines_Part2.ipynb))\n",
    "\n",
    "__Performance indicators:__ Classification accuracy and $R_K$\n",
    "\n",
    "\n",
    "## 1. Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4fbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching, Dataset\n",
    "from transformers import GPT2TokenizerFast, DataCollatorWithPadding, set_seed\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils_ import tokenize, evaluate_acc_rk\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "set_seed(42)\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de5b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokeniser\n",
    "# padding to left because GPT2 uses last token for prediction\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2-medium\", padding_side = 'left', \\\n",
    "                                              padding = True, truncation = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # pad with 'eos' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57213f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data collator - https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "# this is a (callable) helper object that sends batches of data to the model\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding = 'max_length', \\\n",
    "                                         return_tensors = 'pt', max_length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3665ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models and set them in evaluation model, if needed\n",
    "model1 = torch.load('baseline_unfiltered.pth')\n",
    "model2 = torch.load('baseline_random_190k.pth')\n",
    "\n",
    "if model1.training:\n",
    "\n",
    "    model1.eval()\n",
    "\n",
    "if model2.training:\n",
    "\n",
    "    model2.eval()\n",
    "\n",
    "# set up dictionary of the models\n",
    "models = {'Unfiltered': model1, 'Random 190k Subset': model2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9386466",
   "metadata": {},
   "source": [
    "## 2. In-Distribution Evaluation - SNLI test - Zero-Shot\n",
    "### 2.1. Data Read + Pre-Processing\n",
    "- Get SNLI Dataset (test split)\n",
    "- One-hot encode labels\n",
    "- Remove instances without gold standard labels, i.e., label = -1\n",
    "- Tokenise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c8d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/shana92/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f7fbca1d994541a79267851770f7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b8bbd146194f519407ed412d6537e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fcd225adef4ed0ba268b526b6050d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9824 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc68a759c164c81bbc8447d794bc6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in data\n",
    "snli_test = load_dataset('snli', split = 'test')\n",
    "snli_test = snli_test.filter(lambda x: x['label'] != -1).map( \\\n",
    "    lambda x: {'label': one_hot(torch.tensor(x['label']), 3).type(torch.float32).numpy()}, \\\n",
    "    batched = True)\n",
    "\n",
    "# tokenize data\n",
    "snli_test = snli_test.map(lambda x: tokenize(tokenizer, x['premise'] + '|' + x['hypothesis']))\n",
    "len_bef_exclusion = len(snli_test)\n",
    "\n",
    "# exclude instances with > 128 tokens\n",
    "snli_test = snli_test.filter(lambda x: x['exclude'] == False)\n",
    "len_aft_exclusion = len(snli_test)\n",
    "\n",
    "# print message if instances were in fact excluded\n",
    "if len_bef_exclusion - len_aft_exclusion > 0:\n",
    "    \n",
    "    print(f'{len_bef_exclusion - len_aft_exclusion} ' + \\\n",
    "          f'({(len_bef_exclusion/len_aft_exclusion - 1)*100:>2f}%) sequences excluded')\n",
    "\n",
    "# format data as torch tensors\n",
    "snli_test.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5460596",
   "metadata": {},
   "source": [
    "### 2.2. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b809f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa7c5903f2740f0ab9fbe4266c3c10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: SNLI (test) - Accuracy: 89.403504%, RK: 0.841182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fbd289105b4993bc63b8b5838456fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: SNLI (test) - Accuracy: 86.370116%, RK: 0.796408\n"
     ]
    }
   ],
   "source": [
    "# set up dataloader (batch generator)\n",
    "dataloader = torch.utils.data.DataLoader(snli_test, batch_size=128, collate_fn=data_collator)\n",
    "\n",
    "# evaluate models\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    acc, rk = evaluate_acc_rk(model, dataloader, device)\n",
    "    print(f'Model: {model_name} - Dataset: SNLI (test) - Accuracy: {acc*100:>3f}%, RK: {rk:>3f}')\n",
    "\n",
    "# free up some RAM\n",
    "del snli_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41495cad",
   "metadata": {},
   "source": [
    "## 3. Out-of-Distribution Evaluation - HANS - Zero-Shot\n",
    "### 3.1. Data Read + Pre-Processing\n",
    "- Get HANS Dataset (validation split)\n",
    "- One-hot encode labels\n",
    "- Remove instances without gold standard labels, i.e., label = -1\n",
    "- Tokenise data\n",
    "- Partition the data by `heuristic`; categories are `constituent`, `lexical_overlap`, `subsequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e006fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hans (/home/shana92/.cache/huggingface/datasets/hans/plain_text/1.0.0/452e93cf5383f5ae39088254215b517d0da98ccaaf0af8f7ab04d8f23f67dbd9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b969671adf470b9b7de4e37ee4c667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0226683a2b4d3abcc0d1a53aca2126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74905697b93a4b1d9d4cefea982f4411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18530f25a464e96ab4dff6cfec84844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157dda83c8c84144be8186c1033754c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3fa5ec93484ebd96f0ceb9390a1353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9969530e8ed1479da30f6fef32c2f935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in data\n",
    "hans = load_dataset('hans', split = 'validation')\n",
    "hans = hans.filter(lambda x: x['label'] != -1).map( \\\n",
    "    lambda x: {'label': one_hot(torch.tensor(x['label']), 3).type(torch.float32).numpy()}, \\\n",
    "    batched = True)\n",
    "\n",
    "# tokenize data\n",
    "hans = hans.map(lambda x: tokenize(tokenizer, x['premise'] + '|' + x['hypothesis']))\n",
    "len_bef_exclusion = len(hans)\n",
    "\n",
    "# exclude instances with > 128 tokens\n",
    "hans = hans.filter(lambda x: x['exclude'] == False)\n",
    "len_aft_exclusion = len(hans)\n",
    "\n",
    "# print message if instances were in fact excluded\n",
    "if len_bef_exclusion - len_aft_exclusion > 0:\n",
    "    \n",
    "    print(f'{len_bef_exclusion - len_aft_exclusion} ' + \\\n",
    "          f'({(len_bef_exclusion/len_aft_exclusion - 1)*100:>2f}%) sequences excluded')\n",
    "\n",
    "# partition data by `heuristic` \n",
    "data_dict = {x: hans.filter(lambda y: y['heuristic'] == x) \\\n",
    "            for x in ['constituent', 'lexical_overlap', 'subsequence']}\n",
    "\n",
    "# format as torch tensors\n",
    "for val in data_dict.values():\n",
    "    \n",
    "    val.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c9db9",
   "metadata": {},
   "source": [
    "### 3.2. Load model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a97fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80e90bb75ad42a0aa494e22393351c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: constituent - Accuracy: 50.199997%, RK: 0.033393\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6010f673b741ed8cc56a0c943b9059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: constituent - Accuracy: 51.270002%, RK: 0.106970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597611ba012e4b1ab4c0404eabace525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: lexical_overlap - Accuracy: 53.320003%, RK: 0.183194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ac39af756427f990979c1706dfb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: lexical_overlap - Accuracy: 50.010002%, RK: 0.010000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ab83d61a4443ae845fb13767bb52bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: subsequence - Accuracy: 50.629997%, RK: 0.075034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f32cc41b9c42e19dc3ea0a24779066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: subsequence - Accuracy: 49.930000%, RK: -0.026467\n"
     ]
    }
   ],
   "source": [
    "for data_name, data in data_dict.items():\n",
    "    \n",
    "    # set up dataloader (batch generator)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=128, collate_fn=data_collator)\n",
    "\n",
    "    # evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        acc, rk = evaluate_acc_rk(model, dataloader, device, problem = 'TE')\n",
    "        print(f'Model: {model_name} - Dataset: {data_name} - Accuracy: {acc*100:>3f}%, RK: {rk:>3f}')\n",
    "\n",
    "# free up some RAM\n",
    "del hans\n",
    "del data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307de6d",
   "metadata": {},
   "source": [
    "## 4. Out-of-Distribution Evaluation - NLI Diagnostics - Zero-Shot\n",
    "### 4.1. Data Read + Pre-Processing\n",
    "- Get NLI Diagnostics Dataset\n",
    "- One-hot encode labels\n",
    "- Tokenise data\n",
    "- Partition data by heuristic type - `Lexical Semantics`, `Predicate-Argument Structure`, `Logic`, `Knowledge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c5400a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d5e9fac42044a7aba2d23fbc677574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5be70a941b748d28493b0d46dd59b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78355ecdbf1b41f7867860ce26480d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb092fa7d11487598dbb4ed7f4e9534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ccdce0fc9c4b4887d59c5ec991e26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7229d4196d8444958c9daa2d3716702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cbac0c448047a1ba6d7b0dc4ebcf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in data\n",
    "nli_diag = Dataset.from_pandas(pd.read_csv('../raw_data/diagnostic-full.tsv', delimiter = '\\t'))\n",
    "text_label_encoder = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "nli_diag = nli_diag.map( \\\n",
    "    lambda x: {'label': one_hot(torch.tensor(text_label_encoder[x['Label']]), 3).type(torch.float32).numpy()})\n",
    "\n",
    "# tokenize data\n",
    "nli_diag = nli_diag.map(lambda x: tokenize(tokenizer, x['Premise'] + '|' + x['Hypothesis']))\n",
    "len_bef_exclusion = len(nli_diag)\n",
    "\n",
    "# exclude instances with > 128 tokens\n",
    "nli_diag = nli_diag.filter(lambda x: x['exclude'] == False)\n",
    "len_aft_exclusion = len(nli_diag)\n",
    "\n",
    "# print message if instances were in fact excluded\n",
    "if len_bef_exclusion - len_aft_exclusion > 0:\n",
    "    \n",
    "    print(f'{len_bef_exclusion - len_aft_exclusion} ' + \\\n",
    "          f'({(len_bef_exclusion/len_aft_exclusion - 1)*100:>2f}%) sequences excluded')\n",
    "\n",
    "# partition data by heuristic\n",
    "data_dict = {x: nli_diag.filter(lambda y: y[x] is not None) \\\n",
    "            for x in ['Lexical Semantics', 'Predicate-Argument Structure', 'Logic', 'Knowledge']}\n",
    "\n",
    "# format as torch tensors\n",
    "for val in data_dict.values():\n",
    "    \n",
    "    val.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ff2a7",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f51a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ec10e7afb842f084a84f4a9d385d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Lexical Semantics - Accuracy: 53.532606%, RK: 0.287183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c578bc7322fe4b2db293fc944fe9b1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Lexical Semantics - Accuracy: 49.184781%, RK: 0.209301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f88a7246264ad19d2f9ec48394ecdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Predicate-Argument Structure - Accuracy: 61.792451%, RK: 0.343092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2981d099d448088debefd71fbd3d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Predicate-Argument Structure - Accuracy: 58.962262%, RK: 0.283766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f225010cb348f3af1b8afeaac4523f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Logic - Accuracy: 46.153846%, RK: 0.193275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425d3144052840dba8e78080d80f1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Logic - Accuracy: 43.406594%, RK: 0.143971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd22fa6f2b0441ab984a4243bfe236a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Knowledge - Accuracy: 42.957747%, RK: 0.136357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f662c9199334b5e8c7e6c1d592ca82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Knowledge - Accuracy: 38.732395%, RK: 0.109651\n"
     ]
    }
   ],
   "source": [
    "for data_name, data in data_dict.items():\n",
    "    \n",
    "    # set up dataloader (batch generator)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=128, collate_fn=data_collator)\n",
    "\n",
    "    # evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        acc, rk = evaluate_acc_rk(model, dataloader, device)\n",
    "        print(f'Model: {model_name} - Dataset: {data_name} - Accuracy: {acc*100:>3f}%, RK: {rk:>3f}')\n",
    "\n",
    "# free up some RAM\n",
    "del nli_diag\n",
    "del data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bc9b7",
   "metadata": {},
   "source": [
    "## 5. Out-of-Distribution Evaluation - Stress Tests - Zero-Shot\n",
    "### 5.1. Data Read + Pre-Processing\n",
    "- Get Stress Test Datasets\n",
    "- Partition data by heuristic type:\n",
    "    - `Competence` to consist of the datasets `antonym_matched`, `antonym_mismatched`, `quant_hard`\n",
    "    - `Distraction` to consist of the datasets `taut2_matched`, `taut2_mismatched`, `negation_matched`,`negation_mismatched`, `length_mismatch_matched`, `length_mismatch_mismatched`\n",
    "    - `Noise` to consist of the datasets `dev_gram_functionword_swap_perturbed_matched`, `dev_gram_keyboard_matched`, `dev_gram_functionword_swap_perturbed_mismatched`, `dev_gram_swap_mismatched`,\n",
    " `dev_gram_keyboard_mismatched`, `dev_gram_swap_matched`, `dev_gram_contentword_swap_perturbed_mismatched`, `dev_gram_contentword_swap_perturbed_matched`\n",
    "- One-hot encode labels\n",
    "- Tokenise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56d0517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bffa39555eb4e58bbf5bd77365b92ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24071 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f674a4686f9444c92b9d4a5847ad6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24071 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd54a3a5a454c5783255ec639813cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic: Competence - 396 (1.672650%) sequences excluded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c16b476af44b158f0d7c372b17d69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/294705 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20546861183541a5ac46098baaa1bdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/294705 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1434912a637b44fc99a7cee3bb210945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic: Distraction - 1060 (0.360980%) sequences excluded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f96da2e2b104f29b04c6e78c2c26f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333160 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861238c475c947369d34e9de0fde3bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333160 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4160aaba1ecd42c396269f2c0bf2a2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic: Noise - 1070 (0.322202%) sequences excluded\n"
     ]
    }
   ],
   "source": [
    "# load in files from '../stress_tests_datasets.pkl'\n",
    "with open('../stress_tests_datasets.pkl', 'rb') as f:\n",
    "    stress_tests_datasets = pickle.load(f)   \n",
    "\n",
    "# utility function to concatenate datasets and return 'datasets.Dataset' in torch format\n",
    "def conc_prep_datasets(heuristic, key_list):\n",
    "    \n",
    "    # concat datasets\n",
    "    out = stress_tests_datasets[key_list[0]]\n",
    "    \n",
    "    for key in key_list[1:]:\n",
    "        \n",
    "        out = pd.concat([out, stress_tests_datasets[key]], axis = 0)\n",
    "    \n",
    "    # one-hot encode labels\n",
    "    out = Dataset.from_pandas(out).map(lambda x: \\\n",
    "        {'label': one_hot(torch.tensor(text_label_encoder[x['gold_label']]), 3).type(torch.float32).numpy()})\n",
    "    \n",
    "    # tokenize\n",
    "    out = out.map(lambda x: tokenize(tokenizer, x['sentence1'] + '|' + x['sentence2']))\n",
    "    len_bef_exclusion = len(out)\n",
    "    \n",
    "    # exclude instances with > 128 tokens\n",
    "    out = out.filter(lambda x: x['exclude'] == False)\n",
    "    len_aft_exclusion = len(out)\n",
    "\n",
    "    # print message if instances were in fact excluded\n",
    "    if len_bef_exclusion - len_aft_exclusion > 0:\n",
    "\n",
    "        print(f'Heuristic: {heuristic} - {len_bef_exclusion - len_aft_exclusion} ' + \\\n",
    "              f'({(len_bef_exclusion/len_aft_exclusion - 1)*100:>2f}%) sequences excluded')\n",
    "    \n",
    "    # format data as torch tensors\n",
    "    out.set_format(type = 'torch', columns = ['label', 'input_ids', 'attention_mask'])\n",
    "    \n",
    "    return(out)\n",
    "    \n",
    "# partition data by heuristic + pre-process them\n",
    "data_dict = {'Competence': conc_prep_datasets('Competence', \\\n",
    "                                              ['antonym_matched', 'antonym_mismatched', 'quant_hard']), \\\n",
    "             'Distraction': conc_prep_datasets('Distraction', \\\n",
    "                             ['taut2_matched', 'taut2_mismatched', 'negation_matched', 'negation_mismatched', \\\n",
    "                             'length_mismatch_matched', 'length_mismatch_mismatched']), \\\n",
    "             'Noise': conc_prep_datasets('Noise', \\\n",
    "                                         [k for k in stress_tests_datasets.keys() if k.startswith('dev_gram')])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fef03d",
   "metadata": {},
   "source": [
    "### 5.2. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3829fea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe439a6249d4ddc907c727ccc8e660b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Competence - Accuracy: 42.065468%, RK: -0.108413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8be3e77dcc433ea44f1940e1d01636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Competence - Accuracy: 28.764519%, RK: -0.098956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f2315ebfc848f58a944b422d7a081a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Distraction - Accuracy: 58.131415%, RK: 0.377976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d276c677f5344bfda8a7d0a381d4af22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Distraction - Accuracy: 51.099116%, RK: 0.303057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebf8e7c65b84cbd977ef6c2754c9e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unfiltered - Dataset: Noise - Accuracy: 64.691800%, RK: 0.471141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836fcae115d249e6a0170e4c239fb9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random 190k Subset - Dataset: Noise - Accuracy: 57.425696%, RK: 0.374395\n"
     ]
    }
   ],
   "source": [
    "for data_name, data in data_dict.items():\n",
    "    \n",
    "    # set up dataloader (batch generator)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=128, collate_fn=data_collator)\n",
    "\n",
    "    # evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        acc, rk = evaluate_acc_rk(model, dataloader, device)\n",
    "        print(f'Model: {model_name} - Dataset: {data_name} - Accuracy: {acc*100:>3f}%, RK: {rk:>3f}')\n",
    "\n",
    "# free up some RAM\n",
    "del stress_tests_datasets\n",
    "del data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b965daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
